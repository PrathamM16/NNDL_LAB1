{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAB - 1\n",
    "\n",
    "<p>Name: L Vinay Kumar Reddy</p>\n",
    "<p>Regno: 2347126 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.1, epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    # ReLU activation function\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    # Derivative of ReLU \n",
    "    def relu_derivative(self, z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    #Train the perceptron using gradient descent\n",
    "    def train(self, X, y, random_weights=True):\n",
    "        n_samples, n_features = X.shape\n",
    "    \n",
    "        if random_weights:\n",
    "            self.weights = np.random.rand(n_features)  \n",
    "        else:\n",
    "            self.weights = np.array([0.5, 0.5])  \n",
    "        self.bias = 0.0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for idx, x_i in enumerate(X):\n",
    "             \n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                predicted = self.relu(linear_output)\n",
    "                error = y[idx] - predicted\n",
    "                gradient = error * self.relu_derivative(predicted)\n",
    "                self.weights += self.learning_rate * gradient * x_i\n",
    "                self.bias += self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self.relu(linear_output)\n",
    "\n",
    "perceptron = Perceptron(learning_rate=0.1, epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND\n",
    "<p>Truth Table for AND Gate:</p>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Input 1</th>\n",
    "      <th>Input 2</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Results:\n",
      "Input: [0 0] - Predicted: 0 - Actual: 0\n",
      "Input: [0 1] - Predicted: 0 - Actual: 0\n",
      "Input: [1 0] - Predicted: 0 - Actual: 0\n",
      "Input: [1 1] - Predicted: 1 - Actual: 1\n",
      "AND Gate Accuracy: 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AND Gate Implementation\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y_and = np.array([0, 0, 0, 1])  # AND gate outputs\n",
    "\n",
    "perceptron_and = PerceptronSigmoid(learning_rate=0.1, epochs=100)\n",
    "perceptron_and.train(X_and, y_and, random_weights=True)\n",
    "\n",
    "# Test the AND gate perceptron\n",
    "print(\"AND Gate Results:\")\n",
    "correct_predictions = 0\n",
    "for x, target in zip(X_and, y_and):\n",
    "    output = perceptron_and.predict(x)\n",
    "    predicted = 1 if round(output) >= 0.5 else 0  # Convert to binary output\n",
    "    print(f\"Input: {x} - Predicted: {predicted} - Actual: {target}\")\n",
    "    if predicted == target:\n",
    "        correct_predictions += 1\n",
    "accuracy_or = correct_predictions / len(y_and) * 100\n",
    "print(f\"AND Gate Accuracy: {accuracy_or:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions - AND Gate\n",
    "\n",
    "##### How do the weights and bias values change during training for the AND gate?\n",
    "* Initially, the weights and bias are random. As the perceptron encounters training errors, it updates them based on the difference between predicted and actual output (the error), using the learning rate to control the step size.\n",
    "##### Can the perceptron successfully learn the AND logic with a linear decision boundary?\n",
    "* Yes, the AND gate is linearly separable, so a Single Layer Perceptron can successfully learn to classify it with a linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR Gate\n",
    "<p> Truth Table for OR Gate </p>\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Input 1</th>\n",
    "      <th>Input 2</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR Gate Results:\n",
      "Input: [0 0] - Predicted: 1 - Actual: 0\n",
      "Input: [0 1] - Predicted: 1 - Actual: 1\n",
      "Input: [1 0] - Predicted: 1 - Actual: 1\n",
      "Input: [1 1] - Predicted: 1 - Actual: 1\n",
      "OR Gate Accuracy: 75.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR Gate Implementation\n",
    "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y_or = np.array([0, 1, 1, 1])  # OR gate outputs\n",
    "\n",
    "perceptron_or = PerceptronSigmoid(learning_rate=0.1, epochs=100)\n",
    "perceptron_or.train(X_or, y_or, random_weights=True)\n",
    "\n",
    "# Test the OR gate perceptron\n",
    "print(\"OR Gate Results:\")\n",
    "correct_predictions = 0\n",
    "for x, target in zip(X_or, y_or):\n",
    "    output = perceptron_or.predict(x)\n",
    "    predicted = 1 if round(output) >= 0.5 else 0  # Convert to binary output\n",
    "    print(f\"Input: {x} - Predicted: {predicted} - Actual: {target}\")\n",
    "    if predicted == target:\n",
    "        correct_predictions += 1\n",
    "accuracy_or = correct_predictions / len(y_or) * 100\n",
    "print(f\"OR Gate Accuracy: {accuracy_or:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions - OR Gate\n",
    "\n",
    "##### What changes in the perceptron's weights are necessary to represent the OR gate logic?\n",
    "* The weights will adjust to reflect that as long as one of the inputs is 1, the output should be 1. Thus, the weights tend to be positive enough to push the linear combination above the activation threshold in the presence of 1s.\n",
    "\n",
    "##### How does the linear decision boundary look for the OR gate classification?\n",
    "* The decision boundary separates the inputs (0,1), (1,0), and (1,1) from (0,0), representing a linear decision surface where any non-zero input leads to a positive output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND-NOT Gate\n",
    "<p> Truth Table for AND-NOT Gate</p>\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Input 1</th>\n",
    "      <th>Input 2</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND-NOT Gate Results:\n",
      "Input: [0 0] - Predicted: 0 - Actual: 0\n",
      "Input: [0 1] - Predicted: 0 - Actual: 0\n",
      "Input: [1 0] - Predicted: 0 - Actual: 1\n",
      "Input: [1 1] - Predicted: 0 - Actual: 0\n",
      "AND-NOT Gate Accuracy: 75.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AND-NOT Gate Implementation\n",
    "X_andnot = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y_andnot = np.array([0, 0, 1, 0])  # AND-NOT gate outputs\n",
    "\n",
    "perceptron_andnot = PerceptronSigmoid(learning_rate=0.1, epochs=100)\n",
    "perceptron_andnot.train(X_andnot, y_andnot, random_weights=True)\n",
    "\n",
    "# Test the AND-NOT gate perceptron\n",
    "print(\"AND-NOT Gate Results:\")\n",
    "correct_predictions = 0\n",
    "for x, target in zip(X_andnot, y_andnot):\n",
    "    output = perceptron_andnot.predict(x)\n",
    "    predicted = 1 if round(output) >= 0.5 else 0  # Convert to binary output\n",
    "    print(f\"Input: {x} - Predicted: {predicted} - Actual: {target}\")\n",
    "    if predicted == target:\n",
    "        correct_predictions += 1\n",
    "accuracy_andnot = correct_predictions / len(y_andnot) * 100\n",
    "print(f\"AND-NOT Gate Accuracy: {accuracy_andnot:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions - AND-NOT\n",
    "\n",
    "##### What is the perceptron's weight configuration after training for the AND-NOT gate?\n",
    "* The weights will adjust such that the perceptron responds only when the first input is 1 and the second input is 0, reflecting the AND-NOT condition.\n",
    "##### How does the perceptron handle cases where both inputs are 1 or 0?\n",
    "* The perceptron outputs 0 for both these cases, as required by the AND-NOT logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR Gate\n",
    "\n",
    "<p> Truth Table for XOR Gate </p>\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Input 1</th>\n",
    "      <th>Input 2</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate Results:\n",
      "Input: [0 0] - Predicted: 0 - Actual: 0\n",
      "Input: [0 1] - Predicted: 0 - Actual: 1\n",
      "Input: [1 0] - Predicted: 1 - Actual: 1\n",
      "Input: [1 1] - Predicted: 1 - Actual: 0\n",
      "XOR Gate Accuracy: 50.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XOR Gate Implementation\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR gate outputs\n",
    "\n",
    "perceptron_xor = PerceptronSigmoid(learning_rate=0.1, epochs=100)\n",
    "perceptron_xor.train(X_xor, y_xor, random_weights=True)\n",
    "\n",
    "# Test the XOR gate perceptron\n",
    "print(\"XOR Gate Results:\")\n",
    "correct_predictions = 0\n",
    "for x, target in zip(X_xor, y_xor):\n",
    "    output = perceptron_xor.predict(x)\n",
    "    predicted = 1 if round(output) >= 0.5 else 0  # Convert to binary output\n",
    "    print(f\"Input: {x} - Predicted: {predicted} - Actual: {target}\")\n",
    "    if predicted == target:\n",
    "        correct_predictions += 1\n",
    "accuracy_xor = correct_predictions / len(y_xor) * 100\n",
    "print(f\"XOR Gate Accuracy: {accuracy_xor:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe and discuss the perceptron's performance in this scenario.\n",
    "\n",
    "The perceptron will struggle with the XOR gate because XOR is not linearly separable (no single straight line can separate the classes). A single-layer perceptron can only solve linearly separable problems like AND, OR, and AND-NOT. To solve XOR, you'd need a more complex model, such as a multi-layer perceptron (MLP) with non-linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions - XOR Gate\n",
    "\n",
    "##### Why does the Single Layer Perceptron struggle to classify the XOR gate?\n",
    "* XOR is not linearly separable, meaning it cannot be correctly classified by a Single Layer Perceptron because its decision boundary is non-linear.\n",
    "\n",
    "##### What modifications can be made to the neural network model to handle the XOR gate correctly?\n",
    "* A Multi-Layer Perceptron (MLP) with at least one hidden layer can successfully classify XOR by learning non-linear decision boundarie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
